{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_Project_Notebook_chris.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxslCLt2gvebB8kBK5MOqt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maychr/hello-world/blob/master/azubi_evaluation_uni_mannheim/code/Project_Notebook_chris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBTzeaqLtxb0"
      },
      "source": [
        "**Current Status**\n",
        "\n",
        "WIP - waiting for input data\n",
        "\n",
        "**Coming Soon** \n",
        "\n",
        "play with starter model till it works\n",
        "test whether we need 1 model per output or 1 model fits all\n",
        "move over to more sophisticated LSTM models\n",
        "\n",
        "**Done**\n",
        "\n",
        "starter kit\n",
        "upload git\n",
        "define functions for Christopher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFMxPYSzr9Pj"
      },
      "source": [
        "##import packages/libraries\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLCsKlansw7a"
      },
      "source": [
        "##hyperparams\n",
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "num_epochs = 30\n",
        "#trunc_type='post'\n",
        "#padding_type='post'\n",
        "#oov_tok = \"<OOV>\"\n",
        "#training_portion = .8"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BNqF_VaGEmZ"
      },
      "source": [
        "#startermodel #1 model all scores\n",
        "def startermodel(vocab_size, embedding_dim, input_length, epochs): #weights=[embeddings_matrix], trainable=False\n",
        "  model = tf.keras.Sequential([\n",
        "    #model design\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(), #fiddle #MaxPooling1D(pool_size=)\n",
        "    tf.keras.layers.Dense(24, activation='relu'), #fiddle\n",
        "    tf.keras.layers.Dense(5, activation='softmax')]) #5 scores, 5 outputs per scenario\n",
        "    \n",
        "  model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  model.summary()\n",
        "        \n",
        "  # fit network\n",
        "  history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)\n",
        "\n",
        "  # plot history\n",
        "  pyplot.plot(history.history['loss'], label='train')\n",
        "  pyplot.plot(history.history['val_loss'], label='test')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "          \n",
        "  return startermodel, history"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvGr81BssSL-"
      },
      "source": [
        "#sophisticated LSTMmodel #1 model overall score\n",
        "def lstmmodel(vocab_size, embedding_dim, input_length, epochs): #weights=[embeddings_matrix], trainable=False\n",
        "  model = tf.keras.Sequential([\n",
        "  #model design\n",
        "    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False), #should be fine, when embedding_matrix included\n",
        "    tf.keras.layers.Dropout(0.2), #fiddle\n",
        "    tf.keras.layers.Conv1D(64, 5, activation='relu'), #fiddle\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=4), #fiddle, i don't know...\n",
        "    tf.keras.layers.LSTM(64), #fiddle\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid') # 1 score, 1 output per scenario\n",
        "    ])\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  #fit network\n",
        "  history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)\n",
        "\n",
        "  #plot history\n",
        "  pyplot.plot(history.history['loss'], label='train')\n",
        "  pyplot.plot(history.history['val_loss'], label='test')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "\n",
        "  return lstmmodel, history"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6jHYI0rshmM"
      },
      "source": [
        "#please ignore, this is just another way how to define a model ot of the course material\n",
        "#model = Sequential()\n",
        "#model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "#model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(LSTM(100))\n",
        "#model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "#model.add(Dense(total_words, activation='softmax'))\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#print(model.summary())\n",
        "\n",
        "#history = model.fit(predictors, label, epochs=100, verbose=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMmRFLWXrbfP"
      },
      "source": [
        "**Notizen** delete or move later\n",
        "\n",
        "**Frage: 1 model f√ºr alle scores | pro score 1 model ???\n",
        "\n"
      ]
    }
  ]
}